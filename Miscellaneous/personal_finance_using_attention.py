# -*- coding: utf-8 -*-
"""Personal Finance using Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eFoEc58YxLclEEaLpsno4i3u4UqRFdom

# Setting up Workspace

## Importing Libraries
"""

import kagglehub
import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import concatenate
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.layers import (
    Dense, Dropout, LayerNormalization, MultiHeadAttention,Layer, GlobalAveragePooling1D, Input,Reshape, Lambda,Attention
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

"""## Importing Dataset"""

# Download latest version
path = kagglehub.dataset_download("shriyashjagtap/indian-personal-finance-and-spending-habits")

print("Path to dataset files:", path)

for filename in os.listdir(path):
    if filename.endswith(".csv"):
        csv_file_path = os.path.join(path, filename)
        break  # Stop after finding the first CSV file

# Read the CSV file using pandas
df = pd.read_csv(csv_file_path)

df.info()

"""# Preliminary Data Analysis"""

df.iloc[:,:15]

df.iloc[:,16:26]

corr = df.select_dtypes(include=[np.number]).corr()

"""### Visualizing Outliers"""

cols = df.select_dtypes(include=[np.number]).columns.to_list()

figure = plt.figure(figsize=(15, 10))

for i, j in enumerate(cols):
    ax = plt.subplot(5,5,i+1)
    sns.boxplot(x = j, data = df)
plt.tight_layout()
plt.show()

"""* **Distribution:** Many of the variables exhibit a right-skewed distribution, meaning there are a few high values that pull the mean to the right.

* **Outliers:**Several variables have outliers, especially on the right side. These outliers might indicate unusual or extreme values that could influence the analysis.

* **Variability:** The variability (spread) of the data differs across variables. Some variables have a wider range of values than others.
"""

figure = plt.figure(figsize=(15, 10))

for i, j in enumerate(cols):
    ax = plt.subplot(5,5,i+1)
    sns.histplot(x = j, data = df)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 7))
sns.countplot(x='City_Tier', data=df, palette='Pastel1', edgecolor='black')
plt.title('City Tier Distribution', fontsize=18)
plt.xlabel('City Tier', fontsize=14)
plt.ylabel('Number of Users', fontsize=14)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
plt.subplot(1,2,1)
sns.scatterplot(
    data=df,
    x='Income',
    y='Desired_Savings',
    hue='City_Tier',
    palette='RdYlBu',
    alpha=0.6,
    edgecolor=None,
    ax=plt.gca()
)
plt.title('Desired Savings vs Income', fontsize=18)
plt.xlabel('Income (INR)', fontsize=8)
plt.xticks(rotation=45)
plt.xlim(0, 500000)
plt.ylabel('Desired Savings (INR)', fontsize=14)
plt.subplot(1,2,2)
sns.scatterplot(
    data=df,
    x='Disposable_Income',
    y='Desired_Savings',
    hue='City_Tier',
    palette='RdYlBu',
    alpha=0.6,
    edgecolor=None,
    ax=plt.gca()
)
plt.title('Desired Savings vs Disposable Income', fontsize=18)
plt.xlabel('Disposable_Income', fontsize=8)
plt.xticks(rotation=45)
plt.xlim(0, 175000)
plt.ylabel('Desired Savings (INR)', fontsize=14)



plt.legend(title='City Tier', fontsize=12, title_fontsize=14)
plt.tight_layout()
plt.show()

"""### Checking for Collinearity"""

plt.figure(figsize=(15, 10))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='Spectral', cbar=True)

"""# Feature Engineering"""

variables = [
    'Groceries', 'Transport', 'Eating_Out', 'Entertainment',
    'Utilities', 'Healthcare', 'Education', 'Miscellaneous'
    ]

targets = [f'Potential_Savings_{cat}' for cat in variables]

numerical_features = [
    'Income', 'Age', 'Dependents','Insurance', 'Rent', 'Disposable_Income', 'Desired_Savings'
] + variables

expenditures = variables + ['Rent', 'Insurance']

categories = ['Occupation', 'City_Tier']

df['Expenses'] = 0
for exp in expenditures:
  df['Expenses'] += df[exp]

corr = df.drop(columns=expenditures).select_dtypes(include=[np.number]).corr()
plt.figure(figsize=(15, 8))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='Spectral', cbar=True)

"""## Feature Selection"""

df2 = df.copy()

numerical_features = [
    'Income', 'Age', 'Dependents', 'Expenses', 'Disposable_Income', 'Desired_Savings'
]

df2.drop(columns=expenditures, inplace=True)

df2.select_dtypes(include=['object']).columns

encoder = LabelEncoder()
for cat in categories:
  df2[cat] = encoder.fit_transform(df2[cat])

df_features = pd.concat([df2[numerical_features], df2[categories]], axis=1)

scaler = StandardScaler()
scaled = scaler.fit_transform(df_features)

X = scaled
y = df2[targets]

"""# Splitting Dataset for ML"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = df['City_Tier'])

"""# Model Building

## **Attention Layer** from scratch
"""

class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight matrix for attention
        self.W = self.add_weight(
            name='att_weight', shape=(input_shape[-1], 1),
            initializer='glorot_uniform', trainable=True
        )
        # Create a bias term for attention
        self.b = self.add_weight(
            name='att_bias', shape=(input_shape[-1],),
            initializer='zeros', trainable=True
        )
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        # Compute attention scores (e) by applying weight and bias
        e = tf.matmul(x, self.W) + self.b  # Shape: (batch_size, time_steps, 1)
        e = tf.squeeze(e, -1)  # Remove the last dimension, shape: (batch_size, time_steps)

        # Apply softmax to get the attention weights (a)
        a = tf.nn.softmax(e, axis=1)  # Normalize scores across time steps or features
        a = tf.expand_dims(a, -1)  # Expand dims to align with input, shape: (batch_size, time_steps, 1)

        # Compute the weighted sum of inputs (output)
        output = x * a  # Apply attention weights to input, shape: (batch_size, time_steps, features)
        return tf.reduce_sum(output, axis=1)  # Aggregate across the time/feature dimension

def rmse_loss(y_true, y_pred):
  return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))

"""## Forming a FeedForward Network"""

input_dim = X_train.shape[1]
input_layer = Input(shape=(input_dim,))

dense1 = Dense(128, activation='linear')(input_layer)
dropout1 = Dropout(0.2)(dense1)

# Adding a parallel path for width
dense_parallel = Dense(32, activation='linear')(input_layer)

reshaped = Reshape((128, 1))(dense1)

attention_output = AttentionLayer()(reshaped)

dense2 = Dense(64, activation='linear')(attention_output)
dropout2 = Dropout(0.2)(dense2)
dense3 = Dense(32, activation='linear')(dropout2)

# Concatenating with final layer before output
merged_output = concatenate([dense3, dense_parallel])

output_layer = Dense(len(targets), activation='linear')(merged_output)

model = Model(inputs=input_layer, outputs=output_layer)

#Compiling Model
model.compile(optimizer='adam', loss=rmse_loss, metrics=['mae'])

model.summary()

early_stopping = EarlyStopping(
    monitor='val_loss', patience=10, restore_best_weights=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6
)

"""## Fitting Model to Data"""

history = model.fit(
    X_train, y_train,
    epochs=150,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping, reduce_lr],
    verbose = 1
)

test_loss, test_mae = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test MAE: {test_mae:.4f}")

"""# Model Evaluation"""

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.show()

plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('Model MAE')
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error')
plt.legend(loc='upper right')
plt.show()

y_pred = model.predict(X_test)
residuals = y_test - y_pred

y_pred = y_pred.reshape(-1)
residuals = residuals.values.reshape(-1)

sns.scatterplot(x=y_pred, y=residuals, alpha = 0.4)
plt.title('Residuals vs Predictions')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.axhline(0, color='red', linestyle='--')
plt.show()

y_pred = model.predict(X_test)

# Convert predictions and actual values to DataFrames for easier handling
y_test_df = pd.DataFrame(y_test, columns=targets).reset_index(drop=True)
y_pred_df = pd.DataFrame(y_pred, columns=targets)

for col in targets:
    plt.figure(figsize=(6, 6))
    sns.scatterplot(x=y_test_df[col], y=y_pred_df[col], alpha=0.5)
    plt.plot([y_test_df[col].min(), y_test_df[col].max()],
             [y_test_df[col].min(), y_test_df[col].max()],
             'r--')
    plt.title(f'Actual vs Predicted for {col}')
    plt.xlabel('Actual Savings')
    plt.ylabel('Predicted Savings')
    plt.show()