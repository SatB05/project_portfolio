# -*- coding: utf-8 -*-
"""2. HamSpam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L-QUP230Knxpuqo3qcfjyYnNpS-xGiWw

# **Importing Libraries**
"""

#EDA
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Data Preprocessing
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

# Crossvaluation, Metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score
from tensorflow.keras.metrics import Precision, Recall

# ML Model
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

#DL Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

"""# **Loading DataFrame**"""

df = pd.read_csv('/content/Hamspam-1.csv', encoding='latin-1')
df.info()

df.rename(columns={'type': 'target'}, inplace = True)
df.head()

"""# **Data Exploration**"""

df['char_count'] = df['text'].str.len()
df['word_count'] = df['text'].apply(nltk.word_tokenize).str.len()
df['sentence_count'] = df['text'].apply(nltk.sent_tokenize).str.len()

plt.figure(figsize=(15,8))
sns.pairplot(df, hue= 'target', palette= 'pastel')
plt.show()

"""## **Handling Outliers**"""

df = df.query('char_count < 400')

plt.figure(figsize=(15,8))
sns.pairplot(df, hue= 'target', palette= 'pastel')
plt.show()

df.shape

"""# **Feature Engineering**


* ## Tokenization
* ## Removing Stopwords
* ## Stemming & Lemmatization
"""

print(df['text'][:5])

text = "The quick brown fox jumps over the lazy dog."
tokens = nltk.word_tokenize(text)

tagged_tokens = nltk.pos_tag(tokens)
print(tagged_tokens)

# Mapping nltk [Position Tags] to WordNet [Position Tags]
def get_pos_tag(tag):
    pos_map = {'NN': 'n', 'VB': 'v', 'JJ': 'a', 'RB': 'r'}
    return pos_map.get(tag[:2], 'n')

def process_text(text):
    ps = PorterStemmer()
    lmr = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    # Tokenizing text, getting POS tags
    tokens = re.sub('[^a-zA-Z]', ' ', text).lower().split()
    token_tag = nltk.pos_tag(tokens)

    processed_words = []
    for word, tag in token_tag:
        if word not in stop_words:
          if tag.startswith('NN'):  # Apply stemming to nouns
                stemmed_word = ps.stem(word)
                processed_words.append(stemmed_word)
          else:  # Apply lemmatization to other POS
                pos = get_pos_tag(tag)
                lemmatized_word = lmr.lemmatize(word, pos=pos)
                processed_words.append(lemmatized_word)

    # Join processed words into a string
    processed_words = ' '.join(processed_words)

    return processed_words

df['processed_text'] = df['text'].apply(process_text)

df['processed_text']

"""# **Vectorization**"""

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(df['processed_text'])

X = tfidf_matrix.toarray()

label_encoder = LabelEncoder()
df['target'] = label_encoder.fit_transform(df['target'])

y = df['target']

X.dtype

"""# **Train Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=10, shuffle= True, stratify=y)

"""# **Model Building**"""

classifiers = [
    KNeighborsClassifier(),
    MultinomialNB(),
    SVC(),
    RandomForestClassifier(),
    ]

for model in classifiers:
  model.fit(X_train,y_train)

pipeline_dict= {0:'KNeighbors', 1:'NaiveBayes', 2:'SVC', 3:'RandomForest'}

"""## **Model Evaluation**"""

for i, model in enumerate(classifiers):
  cv_score = cross_val_score(model,X_train,y_train,scoring='accuracy',cv=5)
  mean_accuracy = sum(cv_score)/len(cv_score)
  mean_accuracy = mean_accuracy*100
  mean_accuracy = round(mean_accuracy,2)
  print(f'{pipeline_dict[i]}: {mean_accuracy}')

precision = []
recall = []
f1_scores = []
training_accuracy = []
testing_accuracy = []

for c in classifiers:
  test_pred = c.predict(X_test)
  prec = precision_score(y_test, test_pred)
  precision.append(prec)
  rec = recall_score(y_test, test_pred)
  recall.append(rec)
  f1_s = f1_score(y_test, test_pred)
  f1_scores.append(f1_s)
  train_accuracy = model.score(X_train,y_train) * 100
  training_accuracy.append(train_accuracy)
  test_accuracy = model.score(X_test,y_test)  * 100
  testing_accuracy.append(test_accuracy)

"""# **Prediction Result**"""

data = {
    'Model': pipeline_dict.values(),
    'Precision': precision,
    'Recall': recall,
    'F1 Score': f1_scores,
    'Training Accuracy': training_accuracy,
    'Testing Accuracy': testing_accuracy
}

results = pd.DataFrame(data)
results

cls = [pipeline_dict[i] for i in pipeline_dict]
test_preds = [model.predict(X_test) for model in classifiers]

# Create subplots
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))

# Plot each confusion matrix
for i, (ax, test_preds) in enumerate(zip(axes.flatten(), test_preds)):
    cm = confusion_matrix(y_test, test_preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_title(cls[i])
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")

plt.tight_layout()
plt.show()

"""# **DL Model Building**

## Data Reshape
"""

X_new = X.reshape((X.shape[0], 1, X.shape[1]))
X_new_train, X_new_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.25, random_state=10, shuffle= True, stratify=y)

lstm_model = Sequential()
lstm_model.add(LSTM(units = 128, input_shape = (X_new_train.shape[1], X_new_train.shape[2]), return_sequences= False))
lstm_model.add(Dropout(0.3))
lstm_model.add(Dense(units = 1, activation= 'sigmoid'))

lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', 'precision', 'recall'])

print(lstm_model.summary())

lstm_model.fit(X_new_train, y_train, epochs=10, batch_size=64, validation_data=(X_new_test, y_test))

"""# **Model Evaluation**


"""

loss, accuracy, precision_val, recall_val = lstm_model.evaluate(X_new_test, y_test)
lstm_f1_score = 2 * (precision_val * recall_val) / (precision_val + recall_val + 1e-7)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy*100:.4f}")
print(f"Precision: {precision_val:.4f}")
print(f"Recall: {recall_val:.4f}")
print(f"F1 Score: {lstm_f1_score: .4f}")

def binary_coder(value, threshold=0.5):
    return 1 if value >= threshold else 0


lstm_train_predictions = lstm_model.predict(X_new_train)
lstm_train_predictions = np.apply_along_axis(binary_coder, 1, lstm_train_predictions)
lstm_train_acc = accuracy_score(y_train, lstm_train_predictions)*100

new_row = {'Model': 'LSTM', 'Precision': precision_val, 'Recall': recall_val, 'F1 Score': lstm_f1_score,'Training Accuracy': lstm_train_acc, 'Testing Accuracy': accuracy*100}
results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)

"""# **Final Result**"""

results