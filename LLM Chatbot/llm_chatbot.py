# -*- coding: utf-8 -*-
"""7. Chatbot Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hfxtb4WcLzj1KclNb_Sv7pQwWunIRb2j
"""

!pip install langchain langchain-community transformers sentence-transformers tiktoken chromadb --quiet
!pip install auto-gptq optimum accelerate faiss-cpu --quiet

import requests
import re
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np

#Importing Knowledge Base
from langchain.document_loaders import GitbookLoader
import nest_asyncio
import asyncio

from transformers import AutoTokenizer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import AutoModelForCausalLM,AutoTokenizer, pipeline
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain import hub
import optimum
import auto_gptq
import accelerate

nest_asyncio.apply()
loader = GitbookLoader("https://docs.gitbook.com", load_all_paths=True)
documents = loader.load()

print(f"fetched {len(documents)} documents.")

model_name_or_path = "TheBloke/Llama-2-7b-Chat-GPTQ"
model_basename = "gptq_model-4bit-128g"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

token_counts = [len(tokenizer(page.page_content, return_tensors='pt').input_ids[0])
for page in documents]

print(f"""Min: {min(token_counts)}
Avg: {int(sum(token_counts) / len(token_counts))}
Max: {max(token_counts)}""")

def tokenizer_len(text):
  return len(tokenizer(text, return_tensors='pt').input_ids[0])

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 200,
    chunk_overlap = 10,   #chunk_size +/- allowance to arrive at a separator
    length_function=tokenizer_len,
    separators=['\n\n', '\n', ' ', '']
)

chunks = text_splitter.split_documents(documents)

embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

vectorstore = Chroma.from_documents(documents= chunks,
                                    embedding=embeddings_model)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

model = AutoModelForCausalLM.from_pretrained(model_name_or_path,
                                             device_map='auto',
                                             trust_remote_code=False,
                                             revision="main")

query = input('The chatbot will assist you with your queries. ')

query_embedding = embeddings_model.embed_query(query)

retrieved_docs = vectorstore.similarity_search_by_vector(query_embedding)

for doc in retrieved_docs:
    print(doc)

query = "How do I share my projects on Gitbook?"
retrieved_docs = retriever.get_relevant_documents(query)

context = " ".join([doc.page_content for doc in retrieved_docs])
input_text = f"Context: {context}\n\nQuery: {query}"

input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to('cuda')

outputs = model.generate(input_ids, max_length=250)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)