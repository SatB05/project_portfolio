# -*- coding: utf-8 -*-
"""7. LLM Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Oib5X1IS7JK97b-lAa92jhjF40YxDu1

# **Installing Dependencies**
"""

!pip install langchain langchain-community transformers sentence-transformers
!pip install auto-gptq optim accelerate faiss-gpu

"""# **Importing Libraries**"""

#Processing
import requests
import re
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np

#Importing Knowledge Base
from langchain.document_loaders import GitbookLoader
import nest_asyncio
import asyncio

from transformers import AutoTokenizer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm
from uuid import uuid4
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
import optimum
import auto_gptq
from transformers import AutoModelForCausalLM,AutoTokenizer, pipeline
import accelerate

"""# **Loading Knowledge Base**"""

nest_asyncio.apply()
loader = GitbookLoader("https://docs.gitbook.com", load_all_paths=True)
all_pages_data = loader.load()

print(f"fetched {len(all_pages_data)} documents.")

"""# **Loading GPT Model**"""

model_name_or_path = "TheBloke/Llama-2-7b-Chat-GPTQ"
model_basename = "gptq_model-4bit-128g"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)

type(all_pages_data)

token_counts = [len(tokenizer(page.page_content, return_tensors='pt').input_ids[0])
for page in all_pages_data]

print(f"""Min: {min(token_counts)}
Avg: {int(sum(token_counts) / len(token_counts))}
Max: {max(token_counts)}""")

def tokenizer_len(text):
  return len(tokenizer(text, return_tensors='pt').input_ids[0])

doc_text = "\n".join([page.page_content for page in all_pages_data])

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 200,
    chunk_overlap = 10,   #chunk_size +/- allowance to arrive at a separator
    length_function=tokenizer_len,
    separators=['\n\n', '\n', ' ', '']
)

chunks = text_splitter.split_text(doc_text)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 200,
    chunk_overlap = 10,   #chunk_size +/- allowance to arrive at a separator
    length_function=tokenizer_len,
    separators=['\n\n', '\n', ' ', '']
)

chunks = text_splitter.split_documents(all_pages_data)

def clean_text(text):
    # List of patterns or words to exclude
    exclude_patterns = ["Homepage", "Community", "Pricing", "Blog", "Developer Documentation", "Ask or Search"]

    # Filter out chunks that contain these patterns
    for pattern in exclude_patterns:
        if pattern in text:
            return None  # Discard this chunk
    return text

clean_chunks = [clean_text(chunk) for chunk in chunks if clean_text(chunk) is not None]

def post_process_chunk(chunk):
    chunk = re.sub(r'\u2006', ' ', chunk) # Removing unicode characters
    chunk = re.sub(r'\s+', ' ', chunk)  # Replace multiple spaces/newlines
    chunk = re.sub(r'Last updated.*', '', chunk) #Trimming unnecessary content

    return chunk.strip()

!pip install chromadb
import re
from chromadb.api.types import Document

def post_process_chunk(chunk: Document):
    """
    Processes a chunk of text to remove unwanted characters and patterns.

    Args:
        chunk (Document): A Document object containing the text to be processed.

    Returns:
        str: The processed text as a string, or None if the input is not a string or bytes-like object.
    """

    try:
        # Assuming 'page_content' is the key for the actual text content
        chunk_text = chunk.page_content

        # Check if chunk_text is a string or bytes-like object
        if not isinstance(chunk_text, (str, bytes)):
            return None

        chunk_text = re.sub(r'\u2006', ' ', chunk_text)  # Removing unicode characters
        chunk_text = re.sub(r'\s+', ' ', chunk_text)  # Replace multiple spaces/newlines
        chunk_text = re.sub(r'Last updated.*', '', chunk_text)  # Trimming unnecessary content

        return chunk_text.strip()

    except AttributeError:  # Catch attribute errors if chunk doesn't have 'page_content'
        print(f"Error: Input chunk doesn't have a 'page_content' attribute: {chunk}")
        return None

processed_chunks = [post_process_chunk(chunk) for chunk in clean_chunks]

document = " ".join(processed_chunks)

type(document)

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

vectorstore = Chroma.from_documents(documents= chunks,
                                    embedding=embeddings)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

prompt = hub.pull("rlm/rag-prompt")













embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_texts(processed_chunks, embeddings)

query = input('The chatbot will assist you with your queries. ')

# Create query embedding
query_embedding = embeddings.embed_query(query)

# Retrieving chunks based on query
retrieved_docs = vector_store.similarity_search_by_vector(query_embedding, k=3)

# Outputting Result
for doc in retrieved_docs:
    print(doc)



model_name_or_path = "TheBloke/Llama-2-7b-Chat-GPTQ"
model = AutoModelForCausalLM.from_pretrained(model_name_or_path,
                                             device_map='auto',
                                             trust_remote_code=False,
                                             revision="main")

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,
                                          use_fast=True)

# Step 1: Query input
def get_query_input():
    return input("Enter your query: ")

# Step 2: Search for relevant information from FAISS vector store
def search_faiss_vectorstore(query, vector_store, embeddings):
    # Embed the query
    query_embedding = embeddings.embed_query(query)

    # Search for relevant documents in the FAISS index
    docs_and_scores = vector_store.similarity_search_with_score(query, k=3)  # You can adjust 'k' based on your needs
    return docs_and_scores

# Step 3: Use a language model to generate a response based on the retrieved info
def generate_response(retrieved_docs):
    # Concatenate all retrieved documents
    context = " ".join([doc[0].page_content for doc in retrieved_docs])

    # Initialize a HuggingFace model for text generation (You can use a summarization model or GPT-style models)
    generator = pipeline("text-generation", model="gpt2")  # or use any other text-generation model

    # Use the retrieved context to generate a response
    response = generator(context, max_length=200, num_return_sequences=1)
    return response[0]['generated_text']

# Main function to tie everything together
def query_knowledge_base():
    query = get_query_input()
    retrieved_docs = search_faiss_vectorstore(query, vector_store, embeddings)

    if retrieved_docs:
        response = generate_response(retrieved_docs)
        print("Generated Response: ", response)
    else:
        print("No relevant information found.")

# Run the system
query_knowledge_base()

query = input('The chatbot will assist you with your queries. ')
query_embedding = embeddings.embed_query(query)
context = vector_store.similarity_search(
    query_embedding,
    k=3)

cleaned_context=''
for i in context:
  cleaned_context+=i.page_content+'\n\n'

prompt_template=f'''
Please answer the following question with respect to the below context.

Question: {query}

Context: {cleaned_context}
'''

print("\n\n*** Response:")

input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids
output = model.generate(inputs=input_ids,
                        temperature=0.7,
                        do_sample=True,
                        top_p=0.95,
                        top_k=40,
                        max_new_tokens=500)

print(tokenizer.decode(output[0],skip_special_tokens=True))







documents = []
for page in tqdm(all_pages_data):
  url = page.metadata['source']
  hasher.update(url.encode('utf-8'))
  uid = hasher.hexdigest()[:12]
  chunks = text_splitter.split_text(page.page_content)
  for i, chunk in enumerate(chunks):
    documents.append({
        'id':f'{uid}-{i}',
        'text': chunk,
        'source': url
    })

documents_df  = pd.DataFrame.from_records(documents)
documents_df

embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Apply the embedding model to the text column of the DataFrame
documents_df['embedding'] = documents_df['text'].apply(lambda text: embeddings.embed_query(text))

embeddings_list = np.stack(documents_df['embedding'].values)
text_embeddings = list(zip(documents_df['text'], embeddings_list))

# Prepare the FAISS vector store, mapping embeddings with their corresponding IDs
vector_store = FAISS.from_embeddings(
    text_embeddings,  # Pass the list of (text, embedding) tuples
    embeddings,  # Pass the embedding model
    # Consider passing metadatas (ids or other data) if needed
)

embeddings_list = np.stack(documents_df['embedding'].values)
text_embeddings = list(zip(documents_df['text'].tolist(), embeddings_list))
vector_store = FAISS.from_embeddings(text_embeddings, embedding_model,
                                     metadata = documents_df['source'].to_dict(),
                                     ids=documents_df['id'].tolist())

query = input('The chatbot will assist you with your queries. ')
query_embedding = embedding_model.embed_query(query)

search_results = vector_store.similarity_search(query_embedding, k=3)
for search in search_results:
  print(search)

query = input('The chatbot will assist you with your queries. ')

# Create query embedding
query_embedding = embeddings.embed_query(query)

# Retrieving chunks based on query
retrieved_docs = vector_store.similarity_search_by_vector(query_embedding, k=3)

# Outputting Result
for doc in retrieved_docs:
    print(doc)







temp = documents_df['text'][1]
embeds = embedding_model.encode(temp)
len(embeds)



















!pip install -U "pinecone-client[grpc]"

from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key="5fc6c58a-e5d2-4941-a8ca-a7d28dff7e13")
index_name = 'ml-session'

pc.create_index(
    index_name,
    dimension = 384,
    metric = 'dotproduct',
    spec = ServerlessSpec(
        cloud= 'aws',
        region = 'us_east_1'
    )
)

index = pc.Index(index_name)
index.describe_index_stats()

doc = documents_df['text'].tolist()

batch_size = 100

metadatas = []

for i in tqdm(range(0, len(documents_df), batch_size)):
  # get end of batch
  i_end = min(len(documents_df), i+batch_size)
  batch = documents_df.iloc[i:i_end]
  # Get metadata fields for this record
  metadatas = [{
      'source':record['source'],
      'text':record['text']
  } for j, record in batch.iterrows()]
  # Get the list of contexts/docs
  pinecone_documents = batch['text']
  # Create document embeddings
  embeds = embedding_model.encode(list(pinecone_documents)).tolist()
  print(len(embeds))
  # get IDS
  ids = batch['id']
  # add everything to pinecone
  index.upsert(vectors=zip(ids, embeds, metadatas))













doc_text = "\n".join([doc.page_content for doc in docs])

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""],
)


chunks = text_splitter.split_text(doc_text)

def clean_text(text):
    # List of patterns or words to exclude
    exclude_patterns = ["Homepage", "Community", "Pricing", "Blog", "Developer Documentation", "Ask or Search"]

    # Filter out chunks that contain these patterns
    for pattern in exclude_patterns:
        if pattern in text:
            return None  # Discard this chunk
    return text

# Apply cleaning to the chunks
clean_chunks = [clean_text(chunk) for chunk in chunks if clean_text(chunk) is not None]

hasher = hashlib.md5()

documents = []

for doc in tqdm(docs):
  url = doc.metadata['source']
  hasher.update(url.encode('utf-8'))
  uid = hasher.hexdigest()[:12]
  chunks = text_splitter.split_text(doc.page_content)
  for i, chunk in enumerate(processed_chunks):
    documents.append({
        'id':f'{uid}-{i}',
        'text': chunk,
        'source': url
    })

len(documents)

import pandas as pd
documents_df  = pd.DataFrame.from_records(documents)
documents_df



def post_process_chunk(chunk):
    chunk = re.sub(r'\u2006', ' ', chunk) # Removing unicode characters
    chunk = re.sub(r'\s+', ' ', chunk)  # Replace multiple spaces/newlines
    chunk = re.sub(r'Last updated.*', '', chunk) #Trimming unnecessary content

    return chunk.strip()

processed_chunks = [post_process_chunk(chunk) for chunk in clean_chunks]

for chunk in processed_chunks:
    print(chunk)

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_texts(processed_chunks, embeddings)

"""# **Testing**"""

# Sample query input
query = input('The chatbot will assist you with your queries. ')

# Create query embedding
query_embedding = embeddings.embed_query(query)

# Retrieving chunks based on query
retrieved_docs = vector_store.similarity_search_by_vector(query_embedding, k=3)

# Outputting Result
for doc in retrieved_docs:
    print(doc)