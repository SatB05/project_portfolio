# -*- coding: utf-8 -*-
"""+Anxiety.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GxJTC-IiP5848tJsiFIEHcgiuDXPzee8

# **Importing Libraries**
"""

!pip install scikeras --quiet

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

import tensorflow as tf
from sklearn.base import BaseEstimator, ClassifierMixin
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from scikeras.wrappers import KerasClassifier

"""# **Data Exploration**"""

df = pd.read_csv('/content/Anxiety Level in Healthcare Workers Dataset-2 - ML SHEET_PROJECT7.csv')
df.info()

df.head()

"""## **Feature Engineering**"""

def anxiety_class(score):
  if 0 <= score <= 4:
    return 'Minimal'
  elif 5<= score <= 9:
    return 'Mild'
  elif 10<= score <= 14:
    return 'Moderate'
  elif 15<= score <= 21:
    return 'Severe'

df['anxiety_level'] = df['ANXIETY SCORE'].apply(anxiety_class)

df

plt.figure(figsize=(15, 10))
sns.heatmap(df.drop(columns=['Timestamp', 'Name', 'anxiety_level']).corr(),
            annot= True, fmt = '.2f')

plt.pie(df['anxiety_level'].value_counts(),
        labels = df['anxiety_level'].value_counts().index,
        explode = [0,0.01,0.015,0],
        autopct='%1.1f%%')
plt.legend()

"""### **Data Imbalance Handling**

>Given that we have a small dataset with only 221 entries and Severe anxiety cases account for only 8%, it is important to handle the imbalance carefully, as overly aggressive resampling techniques might lead to overfitting or loss of important data.

>> **SMOTE (Synthetic Minority Over-sampling Technique)** generates synthetic samples for the minority class by interpolating between existing samples rather than simply duplicating them. This helps avoid overfitting while balancing the classes.
"""

X = df.iloc[:, 2:10]
y = df['anxiety_level']

smote = SMOTE(sampling_strategy='auto', random_state= 10)
X_resampled, y_resampled = smote.fit_resample(X, y)

plt.pie(y_resampled.value_counts(),
        labels = y_resampled.value_counts().index,
        explode = [0.01,0.01,0.01,0.01],
        autopct='%1.1f%%')
plt.legend()

len(X), len(X_resampled), len(y), len(y_resampled)

y_resampled.value_counts()

le = LabelEncoder()
y = le.fit_transform(y_resampled)

X = X_resampled

"""# Train Test Split"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,
                                                   random_state=10)

"""# **Predictive Modelling**

> 1. Setting up custom ANN
"""

def custom_ann(input_dim):
  ann_model = Sequential([
        Dense(64, activation='relu', input_dim= input_dim),
        Dropout(0.2),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(4, activation='softmax')
    ])
  ann_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  return ann_model
input_dim = X_train.shape[1]

"""> 2. Extablishing Pipeline"""

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression())
])

param_grid = [
    # Logistic Regression
    {'classifier': [LogisticRegression(max_iter=1000)],
     'classifier__C': np.logspace(-3, 3, 15),
     'classifier__penalty': ['l1', 'l2'],
     'classifier__solver': ['liblinear']},

    # Support Vector Classifier (SVC)
    {'classifier': [SVC()],
     'classifier__C': np.logspace(-4, 4, 15),
     'classifier__kernel': ['poly', 'rbf'],
     'classifier__coef0': [0.0, 1.0]},

    #K-Nearest Neighbors
    {'classifier': [KNeighborsClassifier()],
     'classifier__n_neighbors': [3, 5, 7, 9, 11],
     'classifier__weights': ['uniform', 'distance'],
     'classifier__metric': ['euclidean', 'manhattan'],
     'classifier__algorithm': ['ball_tree', 'kd_tree', 'brute']},

    # Naive Bayes
    {'classifier': [GaussianNB()],
     'classifier__var_smoothing': [1e-8, 1e-7, 1e-9, 1e-10, 1e-11]},

    # Random Forest Classifier
    {'classifier': [RandomForestClassifier()],
     'classifier__n_estimators': [50, 100],
     'classifier__max_depth': [None, 10, 20]},

    # XGBoost Classifier
    {'classifier': [XGBClassifier()],
     'classifier__n_estimators': [50, 100],
     'classifier__learning_rate': [0.1, 0.2, 0.3],
     'classifier__max_depth': [6, 8, 10],
     'classifier__subsample': [0.8, 0.9]},

    # ANN Classifier (Keras)
    {'classifier': [KerasClassifier(model= custom_ann, input_dim=input_dim, verbose=1)],
     'classifier__epochs': [64, 100],
     'classifier__batch_size': [15, 30]},
]

"""> 3. Fitting Models & Recording Best Estimators for Each Model"""

results = pd.DataFrame(columns=['Model', 'Training Accuracy', 'Test Accuracy', 'Precision Score', 'Recall Score', 'F1 Score'])

for params in param_grid:
  # setup Pipeline with the current Classifier
  pipeline.set_params(**{k: v[0] for k, v in params.items()})
  grid_search = GridSearchCV(pipeline, param_grid= [params], cv=10, scoring='accuracy', n_jobs=-1)
  grid_search.fit(X_train, y_train)

  # extracting the best metrics and evaluating current Classifier
  best_model = grid_search.best_estimator_
  y_pred = best_model.predict(X_test)
  train_acc = best_model.score(X_train, y_train)*100
  test_acc = best_model.score(X_test, y_test)*100
  model_precision = precision_score(y_test, y_pred, average='micro')
  model_recall = recall_score(y_test, y_pred, average='micro')
  model_f1 = f1_score(y_test,y_pred, average='micro')

  # recording findings into a temporary df
  temp_df = pd.DataFrame({
        'Model': [type(best_model.named_steps['classifier']).__name__],
        'Training Accuracy': [round(train_acc, 2)],
        'Test Accuracy': [round(test_acc, 2)],
        'Precision Score': [round(model_precision, 4)],
        'Recall Score': [round(model_recall, 4)],
        'F1 Score': [round(model_f1, 4)]
    })

  #finalising entry into the findings df
  results = pd.concat([results, temp_df], ignore_index=True)

"""# **Final Result**"""

results